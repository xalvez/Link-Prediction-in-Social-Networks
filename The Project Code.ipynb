{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install torch-geometric\n",
    "\n",
    "import torch_geometric\n",
    "print(torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "# Load the edges\n",
    "edges_df = pd.read_csv('/content/drive/MyDrive/musae_facebook_edges.csv')\n",
    "\n",
    "# Load the node features\n",
    "with open('/content/drive/MyDrive/musae_facebook_features.json', 'r') as f:\n",
    "    features_json = json.load(f)\n",
    "\n",
    "# Find the maximum number of features to set feature size\n",
    "max_features_len = max(len(features) for features in features_json.values())\n",
    "\n",
    "# Assuming that the JSON keys are the string representations of the node IDs\n",
    "node_features = np.zeros((max(max(map(int, features_json.keys())), max(edges_df.max())) + 1, max_features_len))\n",
    "\n",
    "# Populate the feature matrix with features from the JSON file\n",
    "for node_id, features in features_json.items():\n",
    "    node_features[int(node_id), :len(features)] = features\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "node_features = scaler.fit_transform(node_features)\n",
    "node_features_tensor = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "# Create a NetworkX graph\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges_df.values)\n",
    "\n",
    "# Prepare the positive edge index\n",
    "edge_index_np = edges_df.to_numpy().T\n",
    "edge_index_tensor = torch.tensor(edge_index_np, dtype=torch.long)\n",
    "\n",
    "# Generate negative samples\n",
    "neg_edge_index_tensor = negative_sampling(\n",
    "    edge_index=edge_index_tensor,\n",
    "    num_nodes=node_features_tensor.size(0),\n",
    "    num_neg_samples=edge_index_tensor.size(1),\n",
    ")\n",
    "\n",
    "# Filter out any negative edges that do exist in the graph\n",
    "filtered_neg_edge_index = []\n",
    "for u, v in neg_edge_index_tensor.t().tolist():\n",
    "    if not G.has_edge(u, v):\n",
    "        filtered_neg_edge_index.append([u, v])\n",
    "\n",
    "# If we didn't get enough negative samples, resample\n",
    "while len(filtered_neg_edge_index) < edge_index_tensor.size(1):\n",
    "    extra_neg_edge_index = negative_sampling(\n",
    "        edge_index=edge_index_tensor,\n",
    "        num_nodes=node_features_tensor.size(0),\n",
    "        num_neg_samples=edge_index_tensor.size(1) - len(filtered_neg_edge_index),\n",
    "    )\n",
    "    for u, v in extra_neg_edge_index.t().tolist():\n",
    "        if not G.has_edge(u, v) and [u, v] not in filtered_neg_edge_index:\n",
    "            filtered_neg_edge_index.append([u, v])\n",
    "\n",
    "filtered_neg_edge_index = torch.tensor(filtered_neg_edge_index, dtype=torch.long).t()\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "# This should be done before filtering negative samples, based on your edge_index_tensor\n",
    "\n",
    "# Use your own criteria or method to create train_mask and test_mask\n",
    "train_mask, test_mask = train_test_split(range(edge_index_tensor.size(1)), test_size=0.15, random_state=42)\n",
    "\n",
    "train_pos_edge_index = edge_index_tensor[:, train_mask]\n",
    "test_pos_edge_index = edge_index_tensor[:, test_mask]\n",
    "\n",
    "# Here, you will also need to split your negative edges if you're planning to evaluate on them\n",
    "train_neg_edge_index = filtered_neg_edge_index[:, train_mask]\n",
    "test_neg_edge_index = filtered_neg_edge_index[:, test_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GCNConv, GATConv, BatchNorm, LayerNorm, GraphSAGE, GAE\n",
    "\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class EnhancedGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(EnhancedGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = BatchNorm(hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)  # GAT layer\n",
    "        self.bn2 = BatchNorm(hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels, aggr='mean')  # GraphSAGE layer\n",
    "        self.bn3 = BatchNorm(hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, out_channels)  # Final GCN layer\n",
    "        self.bn4 = BatchNorm(out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.bn3(self.conv3(x, edge_index)))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.bn4(self.conv4(x, edge_index))  # No activation before the final layer\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "encoder = EnhancedGCN(in_channels=node_features_tensor.size(1), hidden_channels=64, out_channels=64)\n",
    "model = GAE(encoder)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def compute_scores_and_loss(pos_edge_index, neg_edge_index, node_embeddings):\n",
    "    pos_scores = torch.sigmoid((node_embeddings[pos_edge_index[0]] * node_embeddings[pos_edge_index[1]]).sum(dim=1))\n",
    "    neg_scores = torch.sigmoid((node_embeddings[neg_edge_index[0]] * node_embeddings[neg_edge_index[1]]).sum(dim=1))\n",
    "\n",
    "\n",
    "    scores = torch.cat([pos_scores, neg_scores])\n",
    "    labels = torch.cat([torch.ones(pos_scores.size(0)), torch.zeros(neg_scores.size(0))])\n",
    "\n",
    "    loss = -torch.log(pos_scores + 1e-8).mean() - torch.log(1 - neg_scores + 1e-8).mean()\n",
    "    return scores.detach().cpu().numpy(), labels.detach().cpu().numpy(), loss\n",
    "\n",
    "\n",
    "    # Combine scores and true labels\n",
    "    scores = torch.cat([pos_scores, neg_scores])\n",
    "    labels = torch.cat([torch.ones(pos_scores.size(0)), torch.zeros(neg_scores.size(0))])\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = -torch.log(pos_scores + 1e-8).mean() - torch.log(1 - neg_scores + 1e-8).mean()\n",
    "    return scores.detach().cpu().numpy(), labels.detach().cpu().numpy(), loss\n",
    "\n",
    "# Initialize lists to store the metrics for plotting\n",
    "losses = []\n",
    "auc_scores = []\n",
    "learning_rates = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(610):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    node_embeddings = model(node_features_tensor, edge_index_tensor)\n",
    "    scores, labels, loss = compute_scores_and_loss(train_pos_edge_index, train_neg_edge_index, node_embeddings)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate AUC\n",
    "    train_auc = roc_auc_score(labels, scores)\n",
    "\n",
    "      # Record the loss, AUC, and current learning rate\n",
    "    losses.append(loss.item())\n",
    "    auc_scores.append(train_auc)\n",
    "    learning_rates.append(optimizer.param_groups[0]['lr'])  # Assuming a single parameter group\n",
    "    # Extract the exact loss value as a float\n",
    "    exact_loss_value = loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "         # Print loss and AUC\n",
    "         print(f'Epoch {epoch}: Loss = {exact_loss_value:.7f}, Train AUC = {train_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Loss (left y-axis)\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.plot(range(610), losses, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Create a second y-axis for AUC and learning rate\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('AUC / Learning Rate', color=color)\n",
    "ax2.plot(range(610), auc_scores, color='blue', linestyle='dashed', label='Train AUC')\n",
    "ax2.plot(range(610), learning_rates, color='green', linestyle='dotted', label='Learning Rate')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title('Training Progress Over Epochs')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(0.1, -0.1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(610), losses, color='red')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training AUC\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(610), auc_scores, color='blue', linestyle='dashed')\n",
    "plt.title('Training AUC Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_predictions(node_embeddings, edge_index, threshold=0.6736):\n",
    "        # Calculate scores using the sigmoid function to interpret them as probabilities\n",
    "        scores = torch.sigmoid((node_embeddings[edge_index[0]] * node_embeddings[edge_index[1]]).sum(dim=1))\n",
    "        # Convert scores to binary predictions based on the threshold\n",
    "        predictions = (scores > threshold).int()\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "  import matplotlib.pyplot as plt\n",
    "  import seaborn as sns\n",
    "\n",
    "  def plot_confusion_matrix(cm, class_names):\n",
    "      \"\"\"\n",
    "      Plots a confusion matrix using seaborn's heatmap.\n",
    "\n",
    "      Args:\n",
    "      cm (array): The confusion matrix to be plotted.\n",
    "      class_names (list): The names of the classes.\n",
    "      \"\"\"\n",
    "      plt.figure(figsize=(10, 7))\n",
    "      sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "      plt.xlabel('Predicted labels')\n",
    "      plt.ylabel('True labels')\n",
    "      plt.title('Confusion Matrix')\n",
    "      plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient computation for performance\n",
    "with torch.no_grad():\n",
    "    # Generate node embeddings\n",
    "    node_embeddings = model(node_features_tensor, edge_index_tensor)\n",
    "\n",
    "    # Get predictions for positive test edges\n",
    "    test_pos_predictions = get_predictions(node_embeddings, test_pos_edge_index)\n",
    "\n",
    "    # Get predictions for negative test edges\n",
    "    test_neg_predictions = get_predictions(node_embeddings, test_neg_edge_index)\n",
    "\n",
    "    # Combine positive and negative predictions\n",
    "    test_predictions = torch.cat([test_pos_predictions, test_neg_predictions], dim=0)\n",
    "\n",
    "    # Assuming test_labels is already defined: 1s for positive test edges, 0s for negative\n",
    "    test_labels = torch.cat([torch.ones(test_pos_edge_index.size(1)), torch.zeros(test_neg_edge_index.size(1))])\n",
    "\n",
    "# Convert tensors to NumPy arrays for sklearn compatibility\n",
    "test_labels = test_labels.numpy()\n",
    "test_predictions = test_predictions.numpy()\n",
    "\n",
    "best_threshold = 0.6736\n",
    "best_fp = float('inf')\n",
    "best_conf_matrix = None\n",
    "\n",
    "for threshold in np.arange(0.6736, 1, 0.6736):\n",
    "    test_pos_predictions = get_predictions(node_embeddings, test_pos_edge_index)\n",
    "    test_neg_predictions = get_predictions(node_embeddings, test_neg_edge_index)\n",
    "\n",
    "\n",
    "    test_predictions = torch.cat([test_pos_predictions, test_neg_predictions], dim=0)\n",
    "    test_labels = torch.cat([torch.ones(test_pos_edge_index.size(1)), torch.zeros(test_neg_edge_index.size(1))])\n",
    "\n",
    "    conf_matrix = confusion_matrix(test_labels.numpy(), test_predictions.numpy())\n",
    "    fp = conf_matrix[0][1]  # False positives are in the off-diagonal of the non-edge row\n",
    "    if fp < best_fp:\n",
    "        best_fp = fp\n",
    "        best_threshold = threshold\n",
    "        best_conf_matrix = conf_matrix\n",
    "\n",
    "print(f'Best threshold: {best_threshold} with {best_fp} false positives')\n",
    "\n",
    "# Compute the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "plot_confusion_matrix(best_conf_matrix, class_names=['Non-Edge', 'Edge'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the AUC score\n",
    "test_auc = roc_auc_score(test_labels, test_predictions)\n",
    "\n",
    "# Print the AUC score\n",
    "print(f'Test AUC: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "# Generate scores using the sigmoid function\n",
    "pos_scores = torch.sigmoid((node_embeddings[test_pos_edge_index[0]] * node_embeddings[test_pos_edge_index[1]]).sum(dim=1))\n",
    "neg_scores = torch.sigmoid((node_embeddings[test_neg_edge_index[0]] * node_embeddings[test_neg_edge_index[1]]).sum(dim=1))\n",
    "\n",
    "# Concatenate the scores of the positive and negative edges\n",
    "all_scores = torch.cat([pos_scores, neg_scores], dim=0).numpy()\n",
    "\n",
    "# All labels: ones for positive edges, zeros for negative edges\n",
    "all_labels = torch.cat([torch.ones(pos_scores.size(0)), torch.zeros(neg_scores.size(0))]).numpy()\n",
    "\n",
    "# Make sure that all_scores and all_labels are now of the same length\n",
    "assert len(all_scores) == len(all_labels), \"The scores and labels must have the same length\"\n",
    "\n",
    "# Now compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(all_labels, all_scores)\n",
    "\n",
    "# Compute F1 scores for each threshold\n",
    "f1_scores = 2 * precision * recall / (precision + recall + 1e-10)  # Adding a small constant to avoid division by zero\n",
    "\n",
    "# Locate the index of the largest F1 score\n",
    "best_f1_index = np.argmax(f1_scores[:-1])  # The last value of recall will be 1 and precision will be 0, leading to F1=NaN, so we skip it\n",
    "best_threshold = thresholds[best_f1_index]\n",
    "\n",
    "print(f'Best threshold: {best_threshold:.4f} with F1 Score: {f1_scores[best_f1_index]:.4f}')\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.', label='Precision-Recall Curve')\n",
    "plt.scatter(recall[best_f1_index], precision[best_f1_index], marker='o', color='red', label='Best Threshold')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_true_and_predicted_edges(test_pos_edge_index, test_neg_edge_index, node_embeddings, model, threshold=0.6781):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pos_scores = torch.sigmoid((node_embeddings[test_pos_edge_index[0]] * node_embeddings[test_pos_edge_index[1]]).sum(dim=1))\n",
    "        neg_scores = torch.sigmoid((node_embeddings[test_neg_edge_index[0]] * node_embeddings[test_neg_edge_index[1]]).sum(dim=1))\n",
    "\n",
    "        pos_pred = pos_scores > threshold\n",
    "        neg_pred = neg_scores > threshold\n",
    "\n",
    "    true_edges = [tuple(edge) for edge in test_pos_edge_index.t().tolist()]\n",
    "    predicted_edges = [tuple(edge) for edge, pred in zip(test_pos_edge_index.t().tolist(), pos_pred) if pred]\n",
    "    predicted_edges += [tuple(edge) for edge, pred in zip(test_neg_edge_index.t().tolist(), neg_pred) if pred]\n",
    "\n",
    "    print(\"Predicted Edges (Positive Predictions):\", [tuple(edge) for edge, pred in zip(test_pos_edge_index.t().tolist(), pos_pred) if pred])\n",
    "    print(\"Predicted Edges (Negative Predictions):\", [tuple(edge) for edge, pred in zip(test_neg_edge_index.t().tolist(), neg_pred) if pred])\n",
    "\n",
    "    return true_edges, predicted_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.Graph()\n",
    "edges_df = pd.read_csv('/content/drive/MyDrive/musae_facebook_edges.csv')\n",
    "G = nx.from_pandas_edgelist(edges_df, 'id_1', 'id_2')\n",
    "\n",
    "def visualize_test_edges_with_names(node_id, G, test_pos_edge_index, node_embeddings, model, threshold=0.5, node_name_map=None):\n",
    "    if node_id not in G:\n",
    "        print(f\"Node {node_id} does not exist in the graph.\")\n",
    "        return\n",
    "\n",
    "    # Model to evaluate the test edges\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        scores = torch.sigmoid((node_embeddings[test_pos_edge_index[0]] * node_embeddings[test_pos_edge_index[1]]).sum(dim=1))\n",
    "        predictions = scores > threshold\n",
    "        predicted_edges = test_pos_edge_index[:, predictions].t().numpy()\n",
    "\n",
    "    # Create subgraph with neighbors\n",
    "    neighbors = list(G.neighbors(node_id))\n",
    "    sub_nodes = set([node_id] + neighbors)\n",
    "    sub_G = G.subgraph(sub_nodes)\n",
    "\n",
    "    # Position nodes for visualization\n",
    "    pos = nx.spring_layout(sub_G)\n",
    "\n",
    "    # Get actual edges for these nodes\n",
    "    actual_edges = [(u, v) for u, v in G.edges(sub_nodes) if u in sub_nodes and v in sub_nodes]\n",
    "    true_positive_edges = [(int(u), int(v)) for u, v in predicted_edges if u in sub_nodes and v in sub_nodes and G.has_edge(u, v)]\n",
    "\n",
    "    # Draw nodes and actual edges\n",
    "    nx.draw_networkx_nodes(sub_G, pos, node_color='lightblue', node_size=500)\n",
    "    nx.draw_networkx_edges(sub_G, pos, edgelist=actual_edges, edge_color='green', style='solid')\n",
    "\n",
    "    # Highlight true positive edges in red\n",
    "    nx.draw_networkx_edges(sub_G, pos, edgelist=true_positive_edges, edge_color='red', style='solid')\n",
    "\n",
    "    # Draw labels with names\n",
    "    labels = {n: node_name_map.get(n, n) for n in sub_G.nodes()}  # Use node names if available, else node IDs\n",
    "    nx.draw_networkx_labels(sub_G, pos, labels=labels)\n",
    "\n",
    "    plt.title(f\"Subgraph around Node {node_name_map.get(node_id, node_id)} with Actual and True Positive Links\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_id = 45  # Or any node you are interested in\n",
    "visualize_test_edges_with_names(node_id, G, test_pos_edge_index, node_embeddings, model, node_name_map=node_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the target data that contains the node names\n",
    "node_names_df = pd.read_csv('/content/drive/MyDrive/musae_facebook_target.csv')\n",
    "\n",
    "# Create a dictionary to map node IDs to their corresponding names\n",
    "node_name_map = node_names_df.set_index('id')['page_name'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def draw_model(layers, ax):\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, len(layers) * 2.5)\n",
    "    ax.axis('off')\n",
    "\n",
    "    for i, (layer_type, units) in enumerate(reversed(layers)):\n",
    "        y = i * 2.5\n",
    "        ax.add_patch(patches.Rectangle((2, y), 6, 2, edgecolor='black', facecolor='skyblue'))\n",
    "        ax.text(5, y + 1, f'{layer_type}\\n{units} units', size=12, va='center', ha='center')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "layers = [\n",
    "    (\"Input\", \"in_channels\"),\n",
    "    (\"GCNConv\", \"64\"),\n",
    "    (\"BatchNorm\", \"64\"),\n",
    "    (\"GATConv\", \"64\"),\n",
    "    (\"BatchNorm\", \"64\"),\n",
    "    (\"SAGEConv\", \"64\"),\n",
    "    (\"BatchNorm\", \"64\"),\n",
    "    (\"GCNConv\", \"64\"),\n",
    "    (\"BatchNorm\", \"64\")\n",
    "]\n",
    "draw_model(layers, ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph()\n",
    "\n",
    "# Adding nodes for each layer and operation\n",
    "dot.node('Input', 'Input x, edge_index')\n",
    "dot.node('GCN1', 'GCNConv\\n64 units')\n",
    "dot.node('BN1', 'BatchNorm\\n64 units')\n",
    "dot.node('Relu1', 'ReLU')\n",
    "dot.node('Drop1', 'Dropout (0.5)')\n",
    "dot.node('GAT1', 'GATConv\\n64 units')\n",
    "dot.node('BN2', 'BatchNorm\\n64 units')\n",
    "dot.node('Relu2', 'ReLU')\n",
    "dot.node('Drop2', 'Dropout (0.5)')\n",
    "dot.node('SAGE1', 'SAGEConv\\n64 units')\n",
    "dot.node('BN3', 'BatchNorm\\n64 units')\n",
    "dot.node('Relu3', 'ReLU')\n",
    "dot.node('Drop3', 'Dropout (0.5)')\n",
    "dot.node('GCN2', 'GCNConv\\n64 units')\n",
    "dot.node('BN4', 'BatchNorm\\n64 units')\n",
    "dot.node('Output', 'Output')\n",
    "\n",
    "# Adding edges according to the forward method\n",
    "dot.edge('Input', 'GCN1')\n",
    "dot.edge('GCN1', 'BN1')\n",
    "dot.edge('BN1', 'Relu1')\n",
    "dot.edge('Relu1', 'Drop1')\n",
    "dot.edge('Drop1', 'GAT1')\n",
    "dot.edge('GAT1', 'BN2')\n",
    "dot.edge('BN2', 'Relu2')\n",
    "dot.edge('Relu2', 'Drop2')\n",
    "dot.edge('Drop2', 'SAGE1')\n",
    "dot.edge('SAGE1', 'BN3')\n",
    "dot.edge('BN3', 'Relu3')\n",
    "dot.edge('Relu3', 'Drop3')\n",
    "dot.edge('Drop3', 'GCN2')\n",
    "dot.edge('GCN2', 'BN4')\n",
    "dot.edge('BN4', 'Output')\n",
    "\n",
    "# Render the graph to a file and then display it\n",
    "dot.render('forward_method', format='png', cleanup=True)\n",
    "dot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
